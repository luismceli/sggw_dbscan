---
title: "THESIS Luis Celi"
output: html_document
date: "2024-12-03"
---

```{r}
#Load Library 
library(tidyverse)
library(dplyr)
library(regions)
library(eurostat)
library(ggplot2)
library(regions)
library(stringr)
library(purrr)
library(caret)
library(dbscan)
library(factoextra)
library(sf)
library(giscoR)
library(ggplot2)
library(mclust)
library(dbscan)
library(clusterCrit)
library(factoextra)
library(cluster)
```

Load code for Regions package

```{r}
regiony <- nuts_changes %>%                  #nuts_changes - European Union: Recoded NUTS units 1995-2021
  filter(typology == "nuts_level_1") %>%     #select NUTS 1 regions
  filter(!code_2021 == "NA") %>%             #remove NA observation (we focuse on the NUTS 2010 revision)
  filter(!geo_name_2021 == "NA") %>%
  select(typology, code_2021, geo_name_2021) 

```

Create auxiliary variables to remove Extra-regions, UK regions and French overseas regions

```{r}
regiony$rob1 <- str_sub(regiony$code_2021, 3, 4) #auxiliary variable to remove Extra-regions
regiony$rob2 <- str_sub(regiony$code_2021, 1, 2) #auxiliary variable to remove UK regions

regiony2 <- regiony  %>%         #filtering regions
  filter(!rob1 == "Z") %>%              #remove extra region
  filter(!rob2 == "UK") %>%             #remove UK
  filter(!code_2021 =="FRY") %>%        #remove French overseas regions
  select(code_2021)

regiony2 <- regiony2 %>%
  mutate(geo=code_2021)%>%
  select(geo)

```

Import of wage data from Eurostat (SES 2022) and transformation to GPG

```{r}
wage_h <- get_eurostat("earn_ses22_rhr", time = "num")
```

```{r}
unique(wage_h$TIME_PERIOD)
```

```{r}
wage_h2 <- inner_join(wage_h, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(unit == "EUR") %>%
  filter(!sex =="T") %>%
  select(geo, time, sex, values)
```

```{r}
wage_h <- wage_h2 %>%
  pivot_wider(names_from = sex, values_from = values) %>%
  mutate(gpg = ((M-F)/M)*100) %>%                  #calculation of the gender employment gap
  drop_na() %>%                                    #drop_na() requires tidyverse library
  select(geo, gpg)

rm(wage_h2) #remove the auxiliary data frame
```

Demographic variables

```{r}
popul <- get_eurostat("demo_r_pjanind2", time = "num") 
names(popul)
unique(popul$indic_de)
unique(popul$TIME_PERIOD)
unique(popul$freq)
unique(popul$unit)
```

```{r}
popul2 <- inner_join(popul, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(unit == "PC") %>%
  select(geo, time, indic_de, values)
```

```{r}
popul <- popul2 %>%
    pivot_wider(names_from = indic_de, values_from = values) %>%
    select(geo, DEPRATIO1, DEPRATIO3, OLDDEP1, OLDDEP3, YOUNGDEP1, YOUNGDEP3)

rm(popul2)
```

Labour market variables

Employment rates by sex, age and NUTS 2 region (%)

```{r}
emplyr <- get_eurostat("lfst_r_lfe2emprt", time = "num")
unique(emplyr$unit)
```

```{r}
emplyr2 <- inner_join(emplyr, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(age == "Y20-64") %>%
  filter(!sex == "T") %>%
  select(geo, time, sex, age, values)
```


```{r}
emplyr <- emplyr2 %>%
  pivot_wider(names_from= sex, values_from = values) %>%
  select(geo, M, F) %>%
  mutate(emplygap = M - F) %>%
  mutate(epyr_M = M) %>%
  mutate(epyr_F = F) %>%
  select(geo, emplygap, epyr_M, epyr_F)
  
rm(emplyr2)
```


Unemployment rates by sex, age, educational attainment level and NUTS 2 region (%)

```{r}
unemplyr <- get_eurostat("lfst_r_lfu3rt", time = "num")
unique(unemplyr$isced11)
```
```{r}
unemplyr <- inner_join(unemplyr, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(age == "Y20-64") %>%
  filter(isced11== "TOTAL") %>%
  filter(sex == "T") %>%
  mutate( UMPLR = values ) %>%
  select(geo, UMPLR)
```

Full-time and part-time work in macroregions

```{r}
ftpt <- get_eurostat("lfst_r_lfe2eftpt", time = "num") 
```


```{r}
ftpt2 <- inner_join(ftpt, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(wstatus == "EMP") %>%
  filter(age == "Y20-64") %>%
  select(geo, time, worktime, wstatus, age, sex, values)

```

```{r}
ftpt <- ftpt2 %>%
  pivot_wider(names_from = worktime, values_from = values) %>%
  select (geo, time, wstatus, age, sex, FT, PT, TOTAL) %>%
  mutate(pt_pc = ((PT/TOTAL)*100) ) %>%
  select (geo, sex, pt_pc) %>%
  pivot_wider(names_from = sex, values_from = pt_pc) %>%
  mutate(gptg = M-F ) %>%
  mutate(pt_t = T) %>%
  mutate(pt_f = F) %>%
  mutate(pt_m = M) %>%
  select(geo, gptg, pt_t, pt_f, pt_m)

rm(ftpt2)

```

Activity and InActivity rate in macroregions

```{r}
activ <- get_eurostat("lfst_r_lfp2actrtn", time = "num")
```
```{r}
activ2 <- inner_join(activ, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(age == "Y20-64") %>%
  filter(isced11 == "TOTAL") %>%
  filter(citizen == "TOTAL") %>%
  select(geo, time, sex, values)

```

```{r}
activ <- activ2 %>%
  pivot_wider(names_from = sex, values_from = values) %>%
  mutate(geg = (M-F)) %>%                                 #calculation of the gender employment gap
  mutate(f_inactiv = 100-F) %>%  
  mutate(m_inactiv = 100-M) %>% 
  mutate(f_activ = F) %>%  
  mutate(m_activ = M) %>% 
  mutate(t_activ = T) %>% 
  mutate(t_inactiv = 100-T) %>% 
  drop_na() %>%                                             #drop_na() requires tidyverse library
  select(geo, geg, f_activ, m_activ, t_activ, f_inactiv, m_inactiv, t_inactiv)

rm(activ2)

```

Education

```{r}
edu <- get_eurostat("edat_lfse_04", time = "num") 
```
```{r}
edu2 <- inner_join(edu, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(age == "Y25-64") %>%
   mutate(edu_level =        # We change the sign “-” to another one because the R is treated as a subtraction sign and there are problems with data processing.
      case_when(
        isced11 == "ED0-2" ~ "L",
        isced11 == "ED3-8" ~ "MH",
        isced11 == "ED3_4" ~ "M",
        isced11 == "ED5-8" ~ "H",
        isced11 == "ED3_4GEN" ~ "M1",
        isced11 == "ED3_4VOC" ~ "M2"
              )
          ) %>%
    select(geo, time,age, sex, edu_level, values)

```

```{r}
edu3 <- edu2 %>%
  pivot_wider(names_from = edu_level, values_from = values) %>%
    select(geo, sex, L, M, H)
```


```{r}
edu3L <- edu3 %>%
    select(geo, sex, L) %>%
    pivot_wider(names_from = sex, values_from = L) %>%
    drop_na() %>%
    mutate(TL = T) %>%
    mutate(FL = F) %>%
    mutate(ML = M) %>%
    select(geo, TL, FL, ML)

```


```{r}
edu3M <- edu3 %>%
    select(geo, sex, M) %>%
    pivot_wider(names_from = sex, values_from = M) %>%
    drop_na() %>%
    mutate(TM = T) %>%
    mutate(FM = F) %>%
    mutate(MM = M) %>%
    select(geo, TM, FM, MM)

```

```{r}
edu3H <- edu3 %>%
    select(geo, sex, H) %>%
    pivot_wider(names_from = sex, values_from = H) %>%
    drop_na() %>%
    mutate(TH = T) %>%
    mutate(FH = F) %>%
    mutate(MH = M) %>%
    select(geo, TH, FH, MH)

```

```{r}
edu3 <- inner_join(edu3L, edu3M)
```

```{r}
edu <- inner_join(edu3, edu3H) %>%
  mutate(gedup_l = ML-FL) %>%
  mutate(gedup_m = MM-FM) %>%
  mutate(gedup_h = MH-FH) 
```

```{r}
edu <- inner_join(edu3, edu3H) %>%
  mutate(gedup_l = ML-FL) %>%
  mutate(gedup_m = MM-FM) %>%
  mutate(gedup_h = MH-FH) 

rm(edu2)
rm(edu3)
rm(edu3L)
rm(edu3M)
rm(edu3H)
```

Gross domestic product (GDP) per inhabitatnt

```{r}
gdp <- get_eurostat("nama_10r_2gdp", time = "num") 
names(gdp)
unique(gdp$unit)
```
```{r}
gdp2 <- inner_join(gdp, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  pivot_wider(names_from = unit, values_from = values) %>%
  drop_na() %>%
  mutate(gdp_eur = EUR_HAB) %>%
  mutate(gdp_pps = PPS_EU27_2020_HAB) %>%
  select(geo, gdp_eur, gdp_pps)
```

```{r}
gdp <- gdp2
rm(gdp2)
```


Human Resources in Science & Technology

```{r}
hrst <- get_eurostat("hrst_st_rsex", time = "num")
colnames(hrst)
unique(hrst$category)
unique(hrst$unit)
```

```{r}
hrst2 <- inner_join(hrst, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(category == "HRST") %>%
  filter(unit == "PC_ACT") %>%
  select(geo, sex, values) %>%
  pivot_wider(names_from  = sex , values_from = values) %>%
  mutate(htec_T = T) %>%
  mutate(htec_F = F) %>%
  mutate(htec_M = M) %>%
  mutate(htec_gap = htec_M - htec_F) %>%
  select(geo, htec_T , htec_F, htec_M , htec_gap)

```

```{r}
hrst <- hrst2 
rm(hrst2)
```

Poverty

```{r}
poverty <- get_eurostat("ilc_li41", time = "num")
```
```{r}
poverty <- inner_join(poverty, regiony2) %>%  #merge the preliminary data frame with selected regions 
  mutate(time = TIME_PERIOD) %>%
  filter(time == 2022) %>%
  filter(unit == "PC") %>%
  mutate(poverty = values) %>%
  select(geo, poverty)
```

List of tables to be merged
```{r}
list_tables <- list(wage_h, popul, emplyr, unemplyr, ftpt, activ, edu, gdp, hrst, poverty)
union_df <- Reduce(function(x, y) merge(x, y, by = "geo", all = TRUE), list_tables)
union_df2 <- drop_na(union_df, gpg)
union_df <- union_df2 %>% select(-geo)

geo_info <- union_df2$geo

vars_importantes <- select(union_df, c( gpg, OLDDEP3, pt_m, f_inactiv, gedup_h, gdp_pps)) %>%
  names()
df_data <- union_df %>% select(vars_importantes)

psych::describe(df_data)

rm(list_tables, wage_h, popul, emplyr, unemplyr, ftpt, activ, edu, gdp, poverty, hrst)


```

```{r}
pre_proc <- preProcess(union_df , method = c("medianImpute", "center", "scale"))
df_scaled <- predict(pre_proc, newdata = union_df)
```

```{r}
names(df_scaled)
```


```{r}
vars_importantes <- select(df_scaled, c( gpg, OLDDEP3, pt_m, f_inactiv, gedup_h, gdp_pps)) %>%
  names()
df_scaled_var1 <- df_scaled %>% select(vars_importantes)
rm (df_scaled)

```

```{r}
# guarda todos los histogramas y boxplots en una carpeta con los nombres de las variables

# Cambia el directorio a la carpeta donde quieres guardar los gráficos

setwd("/Users/luisceli/Library/CloudStorage/OneDrive-Personal/Master Big Data/Tesis/Thesis/figuras")
# Crea los histogramas y boxplots para cada variable
for (var in vars_importantes) {
  p1 <- ggplot(df_scaled_var1, aes_string(x = var)) +
    geom_histogram(bins = 30, fill = "blue", alpha = 0.7) +
    labs(title = paste("Histogram of", var), x = var, y = "Frequency") +
    theme_minimal()
  
  p2 <- ggplot(df_scaled_var1, aes_string(y = var)) +
    geom_boxplot(fill = "red", alpha = 0.7) +
    labs(title = paste("Boxplot of", var), y = var) +
    theme_minimal()
  
  ggsave(paste0(var, "_histogram.png"), plot = p1)
  ggsave(paste0(var, "_boxplot.png"), plot = p2)
}


```


```{r}
# Calcula la matriz de correlación
cor_matrix <- cor(df_scaled_var1, use = "complete.obs")

# Extrae solo la columna de correlaciones con 'gpg'
cor_con_gpg <- cor_matrix[, "gpg"]

# Ordena las correlaciones de mayor a menor (absoluto)
cor_con_gpg_ordenado <- sort(cor_con_gpg, decreasing = TRUE)

# Muestra los resultados
print(cor_con_gpg_ordenado)

# Crear un heatmap de la matriz de correlación
library(ggcorrplot)
ggcorrplot(cor_matrix, 
           method = "circle", 
           type = "lower", 
           lab = TRUE, 
           title = "Correlation Matrix Heatmap",
           colors = c("blue", "white", "red"),
           tl.cex = 10,
           tl.col = "black") +
  theme_minimal()

# Guardar la imagen en la carpeta
setwd("/Users/luisceli/Library/CloudStorage/OneDrive-Personal/Master Big Data/Tesis/Thesis/figuras")
ggsave("correlation_matrix_heatmap.png", width = 10, height = 8)

rm (cor_matrix, cor_con_gpg, cor_con_gpg_ordenado)
```

```{r}

```


```{r}
pca <- prcomp(df_scaled_var1)
summary(pca)

```
#graficar como serian los clusters 

```{r}
fviz_eig(pca, addlabels = TRUE, ylim = c(0, 50)) +
  labs(title = "PCA - Eigenvalues")

```


```{r}
eps_values <- seq(1 , 2, by = 0.1)
results <- data.frame(eps = numeric(), clusters = integer(), outliers = integer(), silhouette = numeric())

for (e in eps_values) {
  modelo <- dbscan(df_scaled_var1, eps = e, minPts = 4)
  sil_score <- tryCatch({
    sil <- silhouette(modelo$cluster, dist(df_scaled_var1))
    mean(sil[, 3])
  }, error = function(e) NA)
  
  results <- rbind(results, data.frame(
    eps = e,
    clusters = length(unique(modelo$cluster)) - 1,
    outliers = sum(modelo$cluster == 0),
    silhouette = sil_score
  ))
}

print(results)

rm(results)
```


```{r}
modelo <- dbscan(df_scaled_var1, eps= 1.7 , minPts = 4)

fviz_cluster(modelo, data = df_scaled_var1, stand = FALSE, geom = "point")
```
```{r}
library(plotly)

# Agregamos los clusters al dataset
df_scaled_var1$cluster <- as.factor(modelo$cluster)

# Gráfico 3D usando plotly (elige las columnas 1, 2 y 3 por ejemplo)
fig <- plot_ly(df_scaled_var1, 
               x = ~df_scaled_var1[,1], 
               y = ~df_scaled_var1[,2], 
               z = ~df_scaled_var1[,3], 
               color = ~cluster, 
               colors = "Set1",
               type = "scatter3d", 
               mode = "markers") %>%
  layout(
    title = "3D PCA Cluster Visualization",
    scene = list(
      xaxis = list(title = "Principal Component 1"),
      yaxis = list(title = "Principal Component 2"),
      zaxis = list(title = "Principal Component 3")
    )
  )

fig
```



```{r}
table(modelo$cluster)
```


```{r}
df_clustered_dbscan <- data.frame(geo = geo_info, cluster = modelo$cluster)
```

```{r}
# Gráfico de Silhouette para DBSCAN
sil <- silhouette(modelo$cluster, dist(df_scaled_var1))

# Visualiza silhouette
fviz_silhouette(sil)

```


```{r}
nuts1_map <- get_eurostat_geospatial(
  output_class = "sf",
  resolution = "20",   
  nuts_level = 1,
  year = 2021
)
```

```{r}
# Unir el mapa con los datos de clusters
nuts1_map_clustered <- nuts1_map %>%
  left_join(df_clustered_dbscan, by = c("NUTS_ID" = "geo"))
```


```{r}
ggplot(nuts1_map_clustered) +
  geom_sf(aes(fill = factor(cluster)), color = "white", size = 0.2) +
  scale_fill_brewer(palette = "Set2", na.value = "gray90", name = "Cluster") +
  coord_sf(xlim = c(-25, 40), ylim = c(34, 72), expand = FALSE) +
  theme_minimal() +
  labs(title = "DBSCAN clusters by NUTS 1 region in Europe") +
    theme(
    legend.position = "right",
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank()
  )
```

```{r}
fviz_nbclust(df_scaled_var1, kmeans, method = "wss") +
  labs(title = "Elbow Method", x = "Number of clusters (k)", y = "Within-cluster sum of squares (WSS)")
```
```{r}
fviz_nbclust(df_scaled_var1, kmeans, method = "silhouette") +
  labs(title = "Silhouette Analysis", x = "Number of clusters (k)", y = "Silhouette average")
```

# algorit K-means
```{r}
df_scaled_var1 <- df_scaled_var1 %>% select(-cluster)
kmeans_model <- kmeans(df_scaled_var1, centers = 2, nstart = 25)
fviz_cluster(kmeans_model, data = df_scaled_var1, stand = FALSE, geom = "point")
```

```{r}
# Agrega los clusters al dataframe
df_scaled_var1$cluster <- as.factor(kmeans_model$cluster)

# Gráfico 3D (ajusta las columnas si quieres usar otras dimensiones)
fig <- plot_ly(df_scaled_var1,
               x = ~df_scaled_var1[,1],
               y = ~df_scaled_var1[,2],
               z = ~df_scaled_var1[,3],
               color = ~cluster,
               colors = "Set1",
               type = "scatter3d",
               mode = "markers")

fig
```


# Unir el mapa con los datos de clusters
```{r}
df_clustered_kmeans <- data.frame(geo = geo_info, cluster = kmeans_model$cluster)
```

```{r}
nuts1_map_clustered_kmeans <- nuts1_map %>%
  left_join(df_clustered_kmeans, by = c("NUTS_ID" = "geo"))
```

```{r}
ggplot(nuts1_map_clustered_kmeans) +
  geom_sf(aes(fill = factor(cluster)), color = "white", size = 0.2) +
  scale_fill_brewer(palette = "Set2", na.value = "gray90", name = "Cluster") +
  coord_sf(xlim = c(-25, 40), ylim = c(34, 72), expand = FALSE) +
  theme_minimal() +
  labs(title = "K-means clusters by NUTS 1 region in Europe") +
    theme(
    legend.position = "right",
    axis.text = element_blank(),
    axis.title = element_blank(),
    axis.ticks = element_blank()
  )
```
Evaluate DBscan Vs Kmeans with CHI

```{r}
library(clusterCrit)
library(factoextra)
library(cluster)

df_scaled_var1 <- df_scaled_var1 %>% select(-cluster)

# Create a function to calculate the CH index
calculate_ch_index <- function(data, clustering) {
  data_matrix <- as.matrix(data)
  clustering_int <- as.integer(clustering)
  ch_index <- clusterCrit::intCriteria(data_matrix, clustering_int, c("Calinski_Harabasz"))
  return(ch_index$calinski_harabasz)
}

# Para DBSCAN
ch_index_dbscan <- calculate_ch_index(df_scaled_var1[modelo$cluster != 0, ],
                                      modelo$cluster[modelo$cluster != 0])

# Para K-means
ch_index_kmeans <- calculate_ch_index(df_scaled_var1,
                                      kmeans_model$cluster)

# Imprimir
print(paste("CH index for DBSCAN:", ch_index_dbscan))
print(paste("CH index for K-means:", ch_index_kmeans))

```

Evaluate DBscan Vs Kmeans with Davies-Bouldin 
```{r}
library(clusterCrit)
library(factoextra)
library(cluster)
# Create a function to calculate the DB index
calculate_db_index <- function(data, clustering) {
  data_matrix <- as.matrix(data)
  clustering_int <- as.integer(clustering)
  db_index <- clusterCrit::intCriteria(data_matrix, clustering_int, c("Davies_Bouldin"))
  return(db_index$davies_bouldin)
}
# Para DBSCAN
db_index_dbscan <- calculate_db_index(df_scaled_var1[modelo$cluster != 0, ],
                                       modelo$cluster[modelo$cluster != 0])
# Para K-means
db_index_kmeans <- calculate_db_index(df_scaled_var1,
                                        kmeans_model$cluster)
# Imprimir
print(paste("DB index for DBSCAN:", db_index_dbscan))
print(paste("DB index for K-means:", db_index_kmeans))

```

```{r}
df_gpg <- data.frame(geo = geo_info, gpg = union_df2$gpg)
# Merge con los datos de GPG

df_kmeans_final <- merge(df_clustered_kmeans, df_gpg, by = "geo")
df_dbscan_final <- merge(df_clustered_dbscan, df_gpg, by = "geo")

# Finalmente:
# Calcular la media de gpg por cluster
# K-means
df_kmeans_final %>%
  group_by(cluster) %>%
  summarise(mean_gpg = mean(gpg, na.rm = TRUE)) -> resumen_kmeans
# DBSCAN
df_dbscan_final %>%
  filter(cluster != 0) %>%  # opcional: quitar outliers
  group_by(cluster) %>%
  summarise(mean_gpg = mean(gpg, na.rm = TRUE)) -> resumen_dbscan

```
```{r}
# Agregar clusters a los datos
df_macro_kmeans <- cbind(cluster = kmeans_model$cluster, df_scaled_var1)
df_macro_dbscan <- cbind(cluster = modelo$cluster, df_scaled_var1)
```

```{r}
# Con dplyr para K-means
library(dplyr)

df_macro_kmeans %>%
  group_by(cluster) %>%
  summarise(across(everything(), mean)) -> resumen_kmeans

df_macro_dbscan %>%
  filter(cluster != 0) %>%  # opcional: quitar outliers
  group_by(cluster) %>%
  summarise(across(everything(), mean)) -> resumen_dbscan
```

```{r}

# Agregar clusters a los datos reales (union_df2)
df_real_kmeans <- data.frame(geo = geo_info, cluster_kmeans = kmeans_model$cluster)
df_real_dbscan <- data.frame(geo = geo_info, cluster_dbscan = modelo$cluster)

# Merge con union_df2 para tener variables reales + cluster
df_kmeans_real <- merge(union_df2, df_real_kmeans, by = "geo")
df_dbscan_real <- merge(union_df2, df_real_dbscan, by = "geo")

# Seleccionar solo geo, cluster y variables de interés
df_kmeans_real <- df_kmeans_real %>%
  select(geo, cluster_kmeans, all_of(vars_importantes))

df_dbscan_real <- df_dbscan_real %>%
  select(geo, cluster_dbscan, all_of(vars_importantes))
```

```{r}
# Resumen de las variables por cluster para K-means

library(dplyr)
resumen_real_kmeans <- df_kmeans_real %>%
  group_by(cluster_kmeans) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))

resumen_real_dbscan <- df_dbscan_real %>%
  filter(cluster_dbscan != 0) %>%  # eliminar outliers
  group_by(cluster_dbscan) %>%
  summarise(across(where(is.numeric), mean, na.rm = TRUE))
```


```{r}
# Resumen de las variables por cluster para DBSCAN
View(resumen_real_kmeans)
View(resumen_real_dbscan)
```

```{r}
# describe de df_dbscan_real agrupado por cluster 
library(psych)
describeBy(df_dbscan_real, group = df_dbscan_real$cluster_dbscan)
```



```{r}
# export en csv df_dbscan_real
write.csv(df_dbscan_real, "df_dbscan_real.csv", row.names = FALSE)
```




